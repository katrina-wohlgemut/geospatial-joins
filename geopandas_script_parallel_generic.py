# -*- coding: utf-8 -*-
"""Geopandas_Script_Parallel_generic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZyAJ6jPb8NSBk-SoJ3AGSS23nWdTvyiN

# Spatial Joins with Parallel Processing
Take a points dataframe and a shapefile dataframe and find all points within areas specified by shapefile.

## Import Libraries
"""

# Import required libraries
import sys
import pandas as pd
import datetime

from shapely.geometry import Point
from geopandas import datasets, GeoDataFrame, read_file
import geopandas as gp
import multiprocessing as mp
import numpy as np
import concurrent.futures

"""## Function Definition"""

# create buffer geodataframe
def buffer_CSD(polygon_df, buffer_size):
    buffers = polygon_df.copy(deep=True)
    buffers['geometry'] = polygon_df.buffer(buffer_size*1000)

    return buffers

# Naming convention is a concatenated string of the base name, the buffer size, and the timestamp
def naming_convention():
    current_date = datetime.date.today()
    output_filename = output_filename_base + "_" + str(buffer_size) + "km_" + str(current_date) + ".csv"

    return output_filename

# write out solution to csv
def write_solution(df_solution):
    df_solution.to_csv(naming_convention(), index = False)

def compute_spatialjoin(df_shapefile_CSD, coordinate_points, buffer_size):
    # Define the number of workers to use
    num_workers = mp.cpu_count()

    # Split the df_shapefile_CSD dataframe into smaller chunks
    chunks = np.array_split(df_shapefile_CSD, num_workers)

    # Create a thread pool or process pool with the specified number of workers
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Submit each chunk of df_shapefile_CSD to the executor for processing
        future_to_chunk = {executor.submit(buffer_CSD, chunk, buffer_size): chunk for chunk in chunks}

        # Collect the results from the executor
        results = []
        for future in concurrent.futures.as_completed(future_to_chunk):
            result = future.result()
            results.append(result)

    # Concatenate the results into a single dataframe
    buffers = pd.concat(results)
    df_solution = coordinate_points.sjoin(buffers, how="left", predicate = "within")
    df_solution= df_solution[['Latitude / Latitude', 'Longitude / Longitude', 'geometry', 'CSDNAME']]
    write_solution(df_solution) #Pass df_output

"""## Main Program

## User Input cell below
"""

##Variable Declaration
path_shapefile = 'sample.shp'
path_points = 'data.csv'
path_output_folder = 'path/to/folder'

#User Variables
buffer_list = [0,2,5]
output_filename_base = 'Base'
points_columns = ['ID','Latitude / Latitude', 'Longitude / Longitude']
CSD_columns = ['NAME', 'geometry']

##Read files into shared objects between threads
df_shapefile_CSD = gp.read_file(path_output_folder+path_shapefile, include_fields=CSD_columns)

#Convert to appropriate coordinate system
df_shapefile_CSD = df_shapefile_CSD.to_crs(4326)
df_shapefile_CSD = df_shapefile_CSD.to_crs(3347)

# read in coordinate points
coordinate_points = pd.read_csv(path_output_folder+path_points, usecols=points_columns, encoding='latin-1')
coordinate_points = coordinate_points.drop_duplicates('ID')

# create geodatframe in proper crs
coordinate_points = gp.GeoDataFrame(coordinate_points, geometry=gp.points_from_xy(coordinate_points['Longitude / Longitude'], coordinate_points['Latitude / Latitude']))
coordinate_points = gp.GeoDataFrame(coordinate_points, geometry=coordinate_points['geometry'], crs=4326)
coordinate_points = coordinate_points.to_crs(3347)

# compute spatial join for each buffer size
for buffer_size in buffer_list:
  compute_spatialjoin(df_shapefile_CSD, coordinate_points, buffer_size)